{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, LambdaCallback\n",
        "from keras.layers import Input, Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.resnet import ResNet50"
      ],
      "metadata": {
        "id": "atn8qCGMowcD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit(history):\n",
        "  # Get the final training and validation losses\n",
        "  final_train_loss = history.history['loss'][-1]\n",
        "  final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "  # Print the final losses\n",
        "  print(f'Final Training Loss: {final_train_loss}')\n",
        "  print(f'Final Validation Loss: {final_val_loss}')\n",
        "\n",
        "  # Check for overfitting\n",
        "  if final_val_loss > final_train_loss:\n",
        "      print('The model is overfitting.')\n",
        "  else:\n",
        "      print('The model is not overfitting.')"
      ],
      "metadata": {
        "id": "LIBJZIW5gfcO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "kiyL8XrsmVH_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohamedhanyyy/chest-ctscan-images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5EQS_j9mcFm",
        "outputId": "620434e6-de30-415c-ef1f-d3ba40a8f970"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images\n",
            "License(s): ODbL-1.0\n",
            "Downloading chest-ctscan-images.zip to /content\n",
            " 95% 113M/119M [00:01<00:00, 120MB/s] \n",
            "100% 119M/119M [00:01<00:00, 98.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/chest-ctscan-images.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n"
      ],
      "metadata": {
        "id": "Uey7TcjMmdjo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import DenseNet121, VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Dense, GlobalAveragePooling2D, BatchNormalization, Flatten\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, LambdaCallback\n",
        "from keras.layers import Input, Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.resnet import ResNet50"
      ],
      "metadata": {
        "id": "2GaKpbSCmgGx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Define image size\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Define data directories\n",
        "data_dirs = {\n",
        "    'train': '/content/Data/train',\n",
        "    'test': '/content/Data/test',\n",
        "    'valid': '/content/Data/valid'\n",
        "}\n",
        "\n",
        "# Initialize lists to store data\n",
        "data = {split: {'images': [], 'labels': []} for split in data_dirs}\n",
        "\n",
        "# Iterate over data directories\n",
        "for split, directory in data_dirs.items():\n",
        "    for label in os.listdir(directory):\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for file in tqdm(os.listdir(label_dir), desc=f'Processing {split}/{label}'):\n",
        "            image_path = os.path.join(label_dir, file)\n",
        "            data[split]['images'].append(preprocess_image(image_path))\n",
        "            data[split]['labels'].append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "for split in data:\n",
        "    data[split]['images'] = np.array(data[split]['images']) / 255.0\n",
        "    data[split]['labels'] = np.array(data[split]['labels'])\n",
        "\n",
        "# Extract unique labels present in the data\n",
        "unique_labels = np.unique(np.concatenate([data[split]['labels'] for split in data]))\n",
        "\n",
        "# Convert labels to one-hot encoding using dynamically extracted labels\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "for split in data:\n",
        "    data[split]['labels'] = tf.keras.utils.to_categorical([label_to_index[label] for label in data[split]['labels']], num_classes=len(unique_labels))\n",
        "\n",
        "\n",
        "# Shuffle training data\n",
        "data['train']['images'], data['train']['labels'] = shuffle(data['train']['images'], data['train']['labels'], random_state=42)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_val_split = 0.2\n",
        "val_test_split = 0.5\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(data['train']['images'], data['train']['labels'], test_size=train_val_split, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=val_test_split, random_state=42)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8hhCK8mjOl",
        "outputId": "98bb8aef-bb43-40d3-a723-c1ddce9f09fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train/adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib: 100%|██████████| 195/195 [00:01<00:00, 150.89it/s]\n",
            "Processing train/normal: 100%|██████████| 148/148 [00:01<00:00, 80.36it/s]\n",
            "Processing train/large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa: 100%|██████████| 115/115 [00:00<00:00, 163.83it/s]\n",
            "Processing train/squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa: 100%|██████████| 155/155 [00:00<00:00, 160.99it/s]\n",
            "Processing test/squamous.cell.carcinoma: 100%|██████████| 90/90 [00:00<00:00, 129.61it/s]\n",
            "Processing test/adenocarcinoma: 100%|██████████| 120/120 [00:00<00:00, 125.12it/s]\n",
            "Processing test/large.cell.carcinoma: 100%|██████████| 51/51 [00:00<00:00, 144.71it/s]\n",
            "Processing test/normal: 100%|██████████| 54/54 [00:01<00:00, 52.83it/s]\n",
            "Processing valid/adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib: 100%|██████████| 23/23 [00:00<00:00, 142.31it/s]\n",
            "Processing valid/normal: 100%|██████████| 13/13 [00:00<00:00, 37.86it/s]\n",
            "Processing valid/large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa: 100%|██████████| 21/21 [00:00<00:00, 128.73it/s]\n",
            "Processing valid/squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa: 100%|██████████| 15/15 [00:00<00:00, 117.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Define image size\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Define data directories\n",
        "data_dirs = {\n",
        "    'train': '/content/Data/train',\n",
        "    'test': '/content/Data/test',\n",
        "    'valid': '/content/Data/valid'\n",
        "}\n",
        "\n",
        "# Initialize lists to store data\n",
        "data = {split: {'images': [], 'labels': []} for split in data_dirs}\n",
        "\n",
        "# Iterate over data directories\n",
        "for split, directory in data_dirs.items():\n",
        "    for label in os.listdir(directory):\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for file in tqdm(os.listdir(label_dir), desc=f'Processing {split}/{label}'):\n",
        "            image_path = os.path.join(label_dir, file)\n",
        "            data[split]['images'].append(preprocess_image(image_path))\n",
        "            data[split]['labels'].append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "for split in data:\n",
        "    data[split]['images'] = np.array(data[split]['images']) / 255.0\n",
        "    data[split]['labels'] = np.array(data[split]['labels'])\n",
        "\n",
        "# Extract unique labels present in the data\n",
        "unique_labels = np.unique(np.concatenate([data[split]['labels'] for split in data]))\n",
        "\n",
        "# Convert labels to one-hot encoding using dynamically extracted labels\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "for split in data:\n",
        "    data[split]['labels'] = tf.keras.utils.to_categorical([label_to_index[label] for label in data[split]['labels']], num_classes=len(unique_labels))\n",
        "\n",
        "# Shuffle training data\n",
        "data['train']['images'], data['train']['labels'] = shuffle(data['train']['images'], data['train']['labels'], random_state=42)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_val_split = 0.2\n",
        "val_test_split = 0.5\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(data['train']['images'], data['train']['labels'], test_size=train_val_split, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=val_test_split, random_state=42)\n",
        "\n",
        "# Apply data augmentation\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "12GsfYUXmnKd",
        "outputId": "c11c6f4f-ef07-42dc-f57f-6e3066037443"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train/adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib:  75%|███████▌  | 147/195 [00:01<00:00, 79.59it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-27ec326f38d9>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Processing {split}/{label}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-27ec326f38d9>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Function to preprocess images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilateralFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLORMAP_BONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59j2gQPOmrTg",
        "outputId": "0aeb99c4-6299-44bc-8503-0f8f9e017147"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "SAu48IWwmvVc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import kerastuner as kt\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Define Monte Carlo Dropout CNN model\n",
        "def build_model(hp):\n",
        "    inputs = layers.Input(shape=(150, 150, 3))\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Flatten layer\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Dense layers with Monte Carlo Dropout\n",
        "    for i in range(hp.Int('num_layers', min_value=1, max_value=3, step=1)):\n",
        "        x = layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
        "                         activation='relu')(x)\n",
        "        x = layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define a tuner\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='keras_tuner_dir',\n",
        "                     project_name='monte_carlo_cnn')\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(x_train, y_train,\n",
        "             epochs=10,\n",
        "             validation_data=(x_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Fit the best model\n",
        "start_time = time()\n",
        "history = best_model.fit(x_train, y_train,\n",
        "                         epochs=10,\n",
        "                         validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the best model on test data using Monte Carlo Dropout\n",
        "mc_dropout_model = tf.keras.models.Sequential([best_model])\n",
        "mc_dropout_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "mc_dropout_predictions = mc_dropout_model.predict(x_test, batch_size=None, verbose=1)\n",
        "mc_dropout_accuracy = np.mean(np.argmax(mc_dropout_predictions, axis=-1) == np.argmax(y_test, axis=-1))\n",
        "\n",
        "# Print metrics\n",
        "print(\"Monte Carlo Dropout Test Accuracy:\", mc_dropout_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Monte Carlo Dropout Test Acc: {mc_dropout_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhLmXO-dmw6k",
        "outputId": "996d482c-d445-4cb0-f642-863f2703b5ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 23s]\n",
            "val_accuracy: 0.9016393423080444\n",
            "\n",
            "Best val_accuracy So Far: 0.9016393423080444\n",
            "Total elapsed time: 00h 07m 05s\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 4s 75ms/step - loss: 0.1746 - accuracy: 0.9429 - val_loss: 0.4521 - val_accuracy: 0.8852\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.0695 - accuracy: 0.9816 - val_loss: 0.4555 - val_accuracy: 0.8689\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.0341 - accuracy: 0.9857 - val_loss: 0.4933 - val_accuracy: 0.8689\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.6661 - val_accuracy: 0.8852\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6676 - val_accuracy: 0.8852\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.6209 - val_accuracy: 0.9180\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7936 - val_accuracy: 0.8852\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.0716 - accuracy: 0.9796 - val_loss: 1.1657 - val_accuracy: 0.8689\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0764 - accuracy: 0.9653 - val_loss: 0.5375 - val_accuracy: 0.8689\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0334 - accuracy: 0.9918 - val_loss: 0.4891 - val_accuracy: 0.8361\n",
            "2/2 [==============================] - 1s 694ms/step\n",
            "Monte Carlo Dropout Test Accuracy: 0.8709677419354839\n",
            "Time taken: 13.353516340255737 seconds\n",
            "Training Acc: 0.984489792585373\n",
            "Validation Acc: 0.8770491778850555\n",
            "Monte Carlo Dropout Test Acc: 0.8709677419354839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC17FTOem1pU",
        "outputId": "e99c0cd9-3327-43e1-b7f4-a566ac90fd25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 0.03339618071913719\n",
            "Final Validation Loss: 0.4890919327735901\n",
            "The model is overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Define Monte Carlo Dropout CNN model\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Adjust regularization strength\n",
        "        layers.Dropout(0.8),  # Increase dropout rate\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = build_model()\n",
        "\n",
        "# Data Augmentation (even more aggressive)\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.4,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more aggressive data augmentation\n",
        "start_time = time()\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    epochs=15,  # Increase epochs for more training\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])  # Adjust patience\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbXhUIz7m9k1",
        "outputId": "7bdf57f0-cea0-445e-ea1b-cef6a8046d13"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "15/15 [==============================] - 7s 217ms/step - loss: 4.1159 - accuracy: 0.2882 - val_loss: 2.6271 - val_accuracy: 0.2295\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 4s 242ms/step - loss: 2.3018 - accuracy: 0.3188 - val_loss: 1.7295 - val_accuracy: 0.4426\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 3s 172ms/step - loss: 1.6596 - accuracy: 0.4521 - val_loss: 1.5317 - val_accuracy: 0.4426\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 2s 163ms/step - loss: 1.5587 - accuracy: 0.4389 - val_loss: 1.3899 - val_accuracy: 0.4426\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 4s 250ms/step - loss: 1.3275 - accuracy: 0.4803 - val_loss: 1.4252 - val_accuracy: 0.4426\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 2s 163ms/step - loss: 1.3082 - accuracy: 0.4869 - val_loss: 1.3427 - val_accuracy: 0.5246\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 2s 158ms/step - loss: 1.3487 - accuracy: 0.4716 - val_loss: 1.4240 - val_accuracy: 0.4426\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 2s 165ms/step - loss: 1.2416 - accuracy: 0.5044 - val_loss: 1.2642 - val_accuracy: 0.4918\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 3s 245ms/step - loss: 1.1876 - accuracy: 0.5109 - val_loss: 1.2439 - val_accuracy: 0.4426\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 3s 175ms/step - loss: 1.1949 - accuracy: 0.5197 - val_loss: 1.2992 - val_accuracy: 0.4426\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 2s 162ms/step - loss: 1.1985 - accuracy: 0.4978 - val_loss: 1.1744 - val_accuracy: 0.4426\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 2s 161ms/step - loss: 1.1771 - accuracy: 0.4825 - val_loss: 1.2081 - val_accuracy: 0.4426\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 3s 210ms/step - loss: 1.1908 - accuracy: 0.5000 - val_loss: 1.2849 - val_accuracy: 0.4426\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 1.1467 - accuracy: 0.5229 - val_loss: 1.2472 - val_accuracy: 0.5082\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 2s 164ms/step - loss: 1.1293 - accuracy: 0.5262 - val_loss: 1.1112 - val_accuracy: 0.5082\n",
            "2/2 [==============================] - 0s 363ms/step - loss: 1.1210 - accuracy: 0.5645\n",
            "Test Loss: 1.1209603548049927\n",
            "Test Accuracy: 0.5645161271095276\n",
            "Time taken: 55.191062450408936 seconds\n",
            "Training Acc: 0.4667467216650645\n",
            "Validation Acc: 0.44590164224306744\n",
            "Test Acc: 0.5645161271095276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmDD0CzbpLb7",
        "outputId": "8f19a15c-1751-4463-db2f-8f01fbee4686"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 1.1292692422866821\n",
            "Final Validation Loss: 1.1111506223678589\n",
            "The model is not overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Define Monte Carlo Dropout CNN model with L1 regularization\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001)),  # L1 regularization\n",
        "        layers.Dropout(0.8),  # Regular dropout\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define Monte Carlo Dropout wrapper function\n",
        "class MCDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate):\n",
        "        super(MCDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.keras.layers.Dropout(self.rate)(inputs, training=True) * (1 - self.rate)\n",
        "        return inputs\n",
        "\n",
        "# Instantiate the model\n",
        "model = build_model()\n",
        "\n",
        "# Wrap dropout layers with MCD behavior\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dropout):\n",
        "        layer = MCDropout(layer.rate)\n",
        "\n",
        "# Data Augmentation (even more aggressive)\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.4,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more aggressive data augmentation\n",
        "start_time = time()\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    epochs=15,  # Increase epochs for more training\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])  # Adjust patience\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h897GR1tIGT",
        "outputId": "e732ee83-0219-4453-8bac-4f44c5cb1c82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "15/15 [==============================] - 6s 234ms/step - loss: 28.8122 - accuracy: 0.2860 - val_loss: 13.3486 - val_accuracy: 0.2295\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 3s 195ms/step - loss: 9.2734 - accuracy: 0.2533 - val_loss: 6.0269 - val_accuracy: 0.2459\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 4s 293ms/step - loss: 4.8452 - accuracy: 0.2576 - val_loss: 3.9088 - val_accuracy: 0.2295\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 8s 520ms/step - loss: 3.5612 - accuracy: 0.3057 - val_loss: 3.2132 - val_accuracy: 0.2951\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 3s 205ms/step - loss: 3.1936 - accuracy: 0.3646 - val_loss: 3.1780 - val_accuracy: 0.3770\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 3s 214ms/step - loss: 3.0422 - accuracy: 0.4410 - val_loss: 2.9170 - val_accuracy: 0.4426\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 3s 207ms/step - loss: 3.0766 - accuracy: 0.4585 - val_loss: 2.8229 - val_accuracy: 0.5082\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 2s 164ms/step - loss: 2.8783 - accuracy: 0.4803 - val_loss: 2.9299 - val_accuracy: 0.4918\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 2s 173ms/step - loss: 2.8629 - accuracy: 0.4672 - val_loss: 2.8550 - val_accuracy: 0.4426\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 4s 259ms/step - loss: 2.9080 - accuracy: 0.5066 - val_loss: 2.8580 - val_accuracy: 0.4426\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 3s 166ms/step - loss: 2.7543 - accuracy: 0.4913 - val_loss: 2.6523 - val_accuracy: 0.4754\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 5s 324ms/step - loss: 2.6774 - accuracy: 0.5000 - val_loss: 2.7275 - val_accuracy: 0.4426\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 4s 226ms/step - loss: 2.6575 - accuracy: 0.4875 - val_loss: 2.6763 - val_accuracy: 0.4426\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 4s 230ms/step - loss: 2.5871 - accuracy: 0.5262 - val_loss: 2.7321 - val_accuracy: 0.4426\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 3s 197ms/step - loss: 2.5809 - accuracy: 0.5153 - val_loss: 2.7890 - val_accuracy: 0.4426\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2.8222 - accuracy: 0.5806\n",
            "Test Loss: 2.8222057819366455\n",
            "Test Accuracy: 0.5806451439857483\n",
            "Time taken: 63.100024938583374 seconds\n",
            "Training Acc: 0.42274744908014933\n",
            "Validation Acc: 0.39672131538391114\n",
            "Test Acc: 0.5806451439857483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Define Monte Carlo Dropout CNN model with L1 regularization\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001)),  # L1 regularization\n",
        "        layers.Dropout(0.8),  # Regular dropout\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define Monte Carlo Dropout wrapper function\n",
        "class MCDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate):\n",
        "        super(MCDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.keras.layers.Dropout(self.rate)(inputs, training=True) * (1 - self.rate)\n",
        "        return inputs\n",
        "\n",
        "# Instantiate the model\n",
        "model = build_model()\n",
        "\n",
        "# Wrap dropout layers with MCD behavior\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dropout):\n",
        "        layer = MCDropout(layer.rate)\n",
        "\n",
        "# Data Augmentation (even more aggressive)\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=0.4,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more aggressive data augmentation\n",
        "start_time = time()\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) // 32,\n",
        "                    epochs=15,  # Increase epochs for more training\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])  # Adjust patience\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Hgt6wSw4Cy",
        "outputId": "6db41a50-c96a-46cd-f70f-d46f78b9fe07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "15/15 [==============================] - 34s 2s/step - loss: 30.3517 - accuracy: 0.2511 - val_loss: 13.7554 - val_accuracy: 0.2459\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 29s 2s/step - loss: 9.6015 - accuracy: 0.2555 - val_loss: 6.1570 - val_accuracy: 0.1639\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 29s 2s/step - loss: 4.8377 - accuracy: 0.2751 - val_loss: 3.8034 - val_accuracy: 0.1639\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 27s 2s/step - loss: 3.4784 - accuracy: 0.2969 - val_loss: 3.1265 - val_accuracy: 0.2459\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 27s 2s/step - loss: 3.0412 - accuracy: 0.4017 - val_loss: 2.9339 - val_accuracy: 0.3934\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 3.0334 - accuracy: 0.4410 - val_loss: 3.2536 - val_accuracy: 0.3607\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 34s 2s/step - loss: 2.9571 - accuracy: 0.4672 - val_loss: 2.7742 - val_accuracy: 0.5410\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 29s 2s/step - loss: 2.8491 - accuracy: 0.4651 - val_loss: 3.0685 - val_accuracy: 0.3934\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 2.7934 - accuracy: 0.4934 - val_loss: 2.7362 - val_accuracy: 0.5246\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 29s 2s/step - loss: 2.6583 - accuracy: 0.4956 - val_loss: 2.8097 - val_accuracy: 0.4098\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 2.6114 - accuracy: 0.4869 - val_loss: 2.4810 - val_accuracy: 0.4590\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 31s 2s/step - loss: 2.5881 - accuracy: 0.4688 - val_loss: 2.4984 - val_accuracy: 0.4590\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 2.5590 - accuracy: 0.4694 - val_loss: 2.4825 - val_accuracy: 0.4590\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 2.4355 - accuracy: 0.5066 - val_loss: 2.5166 - val_accuracy: 0.4590\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 28s 2s/step - loss: 2.4854 - accuracy: 0.4738 - val_loss: 2.4593 - val_accuracy: 0.4590\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 2.3677 - accuracy: 0.5323\n",
            "Test Loss: 2.3676600456237793\n",
            "Test Accuracy: 0.5322580933570862\n",
            "Time taken: 529.720751285553 seconds\n",
            "Training Acc: 0.4165483951568604\n",
            "Validation Acc: 0.38251365621884664\n",
            "Test Acc: 0.5322580933570862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAEW4POvucnt",
        "outputId": "658f2f8b-0657-44ac-cac3-88d5e9933612"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 2.4853811264038086\n",
            "Final Validation Loss: 2.4593255519866943\n",
            "The model is not overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes by Backpropagation**"
      ],
      "metadata": {
        "id": "5xAEFi1hxG55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import layers, losses, metrics\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Define the Bayesian Neural Network model using Bayes by Backpropagation\n",
        "def build_bnn_bbb_model(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        tfp.layers.DenseFlipout(128, activation='relu'),\n",
        "        tfp.layers.DenseFlipout(64, activation='relu'),\n",
        "        tfp.layers.DenseFlipout(num_classes)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Instantiate the BNN model\n",
        "input_shape = (150, 150, 3)\n",
        "num_classes = 10  # Assuming 10 classes for classification\n",
        "bnn_model = build_bnn_bbb_model(input_shape, num_classes)\n",
        "\n",
        "# Define the negative log likelihood loss function for Bayesian models\n",
        "neg_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "# Compile the model\n",
        "bnn_model.compile(optimizer='adam', loss=neg_log_likelihood, metrics=['accuracy'])\n",
        "\n",
        "# Train the BNN model\n",
        "start_time = time()\n",
        "bnn_history = bnn_model.fit(x_train, y_train,\n",
        "                            batch_size=32,\n",
        "                            epochs=10,\n",
        "                            validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the BNN model on test data\n",
        "test_loss, test_accuracy = bnn_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(bnn_history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(bnn_history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "HzINfXUKw4X8",
        "outputId": "4321dfd3-8d85-47ec-a73b-b0917ac5f72f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-14-b7e05a9c5a40>\", line 25, in None  *\n        lambda x, rv_x: -rv_x.log_prob(x)\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'log_prob'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b7e05a9c5a40>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train the BNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m bnn_history = bnn_model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     33\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filezh47311j.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, rv_x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filezh47311j.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-14-b7e05a9c5a40>\", line 25, in None  *\n        lambda x, rv_x: -rv_x.log_prob(x)\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'log_prob'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit"
      ],
      "metadata": {
        "id": "Hj6XOZ0QzM4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}