{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oGOa1ltNunH"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "xRblpP9mN9gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohamedhanyyy/chest-ctscan-images"
      ],
      "metadata": {
        "id": "T7HKwXUfOCXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/chest-ctscan-images.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        ""
      ],
      "metadata": {
        "id": "7luSzLuEOD80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import DenseNet121, VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Dense, GlobalAveragePooling2D, BatchNormalization, Flatten\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, LambdaCallback\n",
        "from keras.layers import Input, Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.resnet import ResNet50\n"
      ],
      "metadata": {
        "id": "yXPsz3oOOFp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Define image size\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Define data directories\n",
        "data_dirs = {\n",
        "    'train': '/content/Data/train',\n",
        "    'test': '/content/Data/test',\n",
        "    'valid': '/content/Data/valid'\n",
        "}\n",
        "\n",
        "# Initialize lists to store data\n",
        "data = {split: {'images': [], 'labels': []} for split in data_dirs}\n",
        "\n",
        "# Iterate over data directories\n",
        "for split, directory in data_dirs.items():\n",
        "    for label in os.listdir(directory):\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for file in tqdm(os.listdir(label_dir), desc=f'Processing {split}/{label}'):\n",
        "            image_path = os.path.join(label_dir, file)\n",
        "            data[split]['images'].append(preprocess_image(image_path))\n",
        "            data[split]['labels'].append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "for split in data:\n",
        "    data[split]['images'] = np.array(data[split]['images']) / 255.0\n",
        "    data[split]['labels'] = np.array(data[split]['labels'])\n",
        "\n",
        "# Extract unique labels present in the data\n",
        "unique_labels = np.unique(np.concatenate([data[split]['labels'] for split in data]))\n",
        "\n",
        "# Convert labels to one-hot encoding using dynamically extracted labels\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "for split in data:\n",
        "    data[split]['labels'] = tf.keras.utils.to_categorical([label_to_index[label] for label in data[split]['labels']], num_classes=len(unique_labels))\n",
        "\n",
        "\n",
        "# Shuffle training data\n",
        "data['train']['images'], data['train']['labels'] = shuffle(data['train']['images'], data['train']['labels'], random_state=42)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_val_split = 0.2\n",
        "val_test_split = 0.5\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(data['train']['images'], data['train']['labels'], test_size=train_val_split, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=val_test_split, random_state=42)\n"
      ],
      "metadata": {
        "id": "416y8BOcOHhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Define image size\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Define data directories\n",
        "data_dirs = {\n",
        "    'train': '/content/Data/train',\n",
        "    'test': '/content/Data/test',\n",
        "    'valid': '/content/Data/valid'\n",
        "}\n",
        "\n",
        "# Initialize lists to store data\n",
        "data = {split: {'images': [], 'labels': []} for split in data_dirs}\n",
        "\n",
        "# Iterate over data directories\n",
        "for split, directory in data_dirs.items():\n",
        "    for label in os.listdir(directory):\n",
        "        label_dir = os.path.join(directory, label)\n",
        "        for file in tqdm(os.listdir(label_dir), desc=f'Processing {split}/{label}'):\n",
        "            image_path = os.path.join(label_dir, file)\n",
        "            data[split]['images'].append(preprocess_image(image_path))\n",
        "            data[split]['labels'].append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "for split in data:\n",
        "    data[split]['images'] = np.array(data[split]['images']) / 255.0\n",
        "    data[split]['labels'] = np.array(data[split]['labels'])\n",
        "\n",
        "# Extract unique labels present in the data\n",
        "unique_labels = np.unique(np.concatenate([data[split]['labels'] for split in data]))\n",
        "\n",
        "# Convert labels to one-hot encoding using dynamically extracted labels\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "for split in data:\n",
        "    data[split]['labels'] = tf.keras.utils.to_categorical([label_to_index[label] for label in data[split]['labels']], num_classes=len(unique_labels))\n",
        "\n",
        "# Shuffle training data\n",
        "data['train']['images'], data['train']['labels'] = shuffle(data['train']['images'], data['train']['labels'], random_state=42)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_val_split = 0.2\n",
        "val_test_split = 0.5\n",
        "x_train, x_val_test, y_train, y_val_test = train_test_split(data['train']['images'], data['train']['labels'], test_size=train_val_split, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=val_test_split, random_state=42)\n",
        "\n",
        "# Apply data augmentation\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "w9ffEfXTOLBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "LN7BEIQwOlpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels)\n"
      ],
      "metadata": {
        "id": "_GApln0SRB5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import kerastuner as kt\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Define Monte Carlo Dropout CNN model\n",
        "def build_model(hp):\n",
        "    inputs = layers.Input(shape=(150, 150, 3))\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Flatten layer\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Dense layers with Monte Carlo Dropout\n",
        "    for i in range(hp.Int('num_layers', min_value=1, max_value=3, step=1)):\n",
        "        x = layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
        "                         activation='relu')(x)\n",
        "        x = layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define a tuner\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='keras_tuner_dir',\n",
        "                     project_name='monte_carlo_cnn')\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(x_train, y_train,\n",
        "             epochs=10,\n",
        "             validation_data=(x_val, y_val))\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Fit the best model\n",
        "start_time = time()\n",
        "history = best_model.fit(x_train, y_train,\n",
        "                         epochs=10,\n",
        "                         validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the best model on test data using Monte Carlo Dropout\n",
        "mc_dropout_model = tf.keras.models.Sequential([best_model])\n",
        "mc_dropout_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "mc_dropout_predictions = mc_dropout_model.predict(x_test, batch_size=None, verbose=1)\n",
        "mc_dropout_accuracy = np.mean(np.argmax(mc_dropout_predictions, axis=-1) == np.argmax(y_test, axis=-1))\n",
        "\n",
        "# Print metrics\n",
        "print(\"Monte Carlo Dropout Test Accuracy:\", mc_dropout_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Monte Carlo Dropout Test Acc: {mc_dropout_accuracy}')"
      ],
      "metadata": {
        "id": "1oAn28gPOrO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}