{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQrDMKjc7XnD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3kX21CmjFC6",
        "outputId": "0c6d10c3-fe7f-4ea4-89e2-661ccb77376f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading brain-tumor-mri-dataset.zip to /content\n",
            " 90% 133M/149M [00:00<00:00, 155MB/s]\n",
            "100% 149M/149M [00:01<00:00, 154MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd3raPAVtFNH"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/brain-tumor-mri-dataset.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/brain_tumor_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKR10oDtjYDw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, LambdaCallback\n",
        "from keras.layers import Input, Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.resnet import ResNet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9djx6QR3kICG",
        "outputId": "dc1ac864-6262-4e18-f327-010109bebc56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1321/1321 [00:04<00:00, 286.70it/s]\n",
            "100%|██████████| 300/300 [00:00<00:00, 305.23it/s]\n",
            "100%|██████████| 1339/1339 [00:04<00:00, 294.39it/s]\n",
            "100%|██████████| 306/306 [00:00<00:00, 310.97it/s]\n",
            "100%|██████████| 1595/1595 [00:03<00:00, 450.11it/s]\n",
            "100%|██████████| 405/405 [00:00<00:00, 549.21it/s]\n",
            "100%|██████████| 1457/1457 [00:05<00:00, 274.97it/s]\n",
            "100%|██████████| 300/300 [00:01<00:00, 226.08it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define labels and image size\n",
        "labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Load images and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for label in labels:\n",
        "    trainPath = os.path.join('/content/brain_tumor_dataset/Training', label)\n",
        "    for file in tqdm(os.listdir(trainPath)):\n",
        "        image_path = os.path.join(trainPath, file)\n",
        "        x_train.append(preprocess_image(image_path))\n",
        "        y_train.append(labels.index(label))\n",
        "\n",
        "    testPath = os.path.join('/content/brain_tumor_dataset/Testing', label)\n",
        "    for file in tqdm(os.listdir(testPath)):\n",
        "        image_path = os.path.join(testPath, file)\n",
        "        x_test.append(preprocess_image(image_path))\n",
        "        y_test.append(labels.index(label))\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "x_train = np.array(x_train) / 255.0\n",
        "x_test = np.array(x_test) / 255.0\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Shuffle training data\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
        "\n",
        "# Perform one-hot encoding on labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCQc225jtufn"
      },
      "source": [
        "**Simple CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsZz7wlBtbKW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 3)),  # Adjust input shape for RGB images\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),  # You can adjust the learning rate if needed\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmipeMB-r6PG",
        "outputId": "f69aec3b-b9e2-4c61-a437-612100785a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 260s 2s/step - loss: 0.8481 - accuracy: 0.6586 - val_loss: 0.5307 - val_accuracy: 0.8031\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 249s 2s/step - loss: 0.4652 - accuracy: 0.8295 - val_loss: 0.4043 - val_accuracy: 0.8443\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 253s 2s/step - loss: 0.3836 - accuracy: 0.8518 - val_loss: 0.3502 - val_accuracy: 0.8705\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 250s 2s/step - loss: 0.2933 - accuracy: 0.8949 - val_loss: 0.2843 - val_accuracy: 0.8933\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 250s 2s/step - loss: 0.2382 - accuracy: 0.9140 - val_loss: 0.2283 - val_accuracy: 0.9151\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 256s 2s/step - loss: 0.1800 - accuracy: 0.9348 - val_loss: 0.2257 - val_accuracy: 0.9195\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 251s 2s/step - loss: 0.1465 - accuracy: 0.9453 - val_loss: 0.1941 - val_accuracy: 0.9370\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 248s 2s/step - loss: 0.1040 - accuracy: 0.9606 - val_loss: 0.2243 - val_accuracy: 0.9326\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 245s 2s/step - loss: 0.0966 - accuracy: 0.9680 - val_loss: 0.1683 - val_accuracy: 0.9475\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 243s 2s/step - loss: 0.0812 - accuracy: 0.9720 - val_loss: 0.2188 - val_accuracy: 0.9344\n",
            "41/41 [==============================] - 19s 456ms/step - loss: 0.2037 - accuracy: 0.9382\n",
            "Test Loss: 0.20369049906730652\n",
            "Test Accuracy: 0.9382150769233704\n",
            "Time taken: 2507.9843294620514 seconds\n",
            "Training Acc: 0.8929525077342987\n",
            "Validation Acc: 0.8997375249862671\n",
            "Test Acc: 0.9382150769233704\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "# Train the model\n",
        "start_time = time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKTvErG3f0q"
      },
      "source": [
        "**Bayes by Backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djSTD7qNN3Ge",
        "outputId": "47082c92-5a25-4a19-e4bb-b8f8d8f9bc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "TensorFlow Probability version: 0.23.0\n",
            "Keras version: 2.15.0\n",
            "NumPy version: 1.25.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow Probability version:\", tfp.__version__)\n",
        "print(\"Keras version:\", keras.__version__)\n",
        "print(\"NumPy version:\", np.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbtuwStovame",
        "outputId": "b175ae8a-0153-47c3-d32c-ffbae2c194f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 422s 3s/step - loss: 0.6409 - accuracy: 0.7649 - val_loss: 1.5751 - val_accuracy: 0.4436\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 426s 3s/step - loss: 0.4595 - accuracy: 0.8803 - val_loss: 1.0882 - val_accuracy: 0.5599\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 430s 3s/step - loss: 0.3911 - accuracy: 0.9230 - val_loss: 0.5424 - val_accuracy: 0.7664\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 430s 3s/step - loss: 0.3436 - accuracy: 0.9525 - val_loss: 0.4509 - val_accuracy: 0.8434\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 428s 3s/step - loss: 0.3227 - accuracy: 0.9610 - val_loss: 0.3831 - val_accuracy: 0.9081\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 427s 3s/step - loss: 0.2975 - accuracy: 0.9709 - val_loss: 0.3583 - val_accuracy: 0.9178\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 426s 3s/step - loss: 0.2718 - accuracy: 0.9858 - val_loss: 0.3726 - val_accuracy: 0.9081\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 429s 3s/step - loss: 0.2575 - accuracy: 0.9899 - val_loss: 0.3354 - val_accuracy: 0.9449\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 427s 3s/step - loss: 0.2505 - accuracy: 0.9902 - val_loss: 0.3523 - val_accuracy: 0.9256\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 426s 3s/step - loss: 0.2419 - accuracy: 0.9950 - val_loss: 0.3278 - val_accuracy: 0.9458\n",
            "41/41 [==============================] - 39s 956ms/step - loss: 0.3151 - accuracy: 0.9451\n",
            "Test Loss: 0.3150804340839386\n",
            "Test Accuracy: 0.9450801014900208\n",
            "Time taken: 4272.999568462372 seconds\n",
            "Training Acc: 0.9949660897254944\n",
            "Validation Acc: 0.9457567930221558\n",
            "Test Acc: 0.9450801014900208\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, regularizers, models, optimizers\n",
        "from time import time\n",
        "\n",
        "# Define Bayesian Neural Network model with increased capacity and regularization\n",
        "def build_bcnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(4, activation=\"sigmoid\")(x)  # Adjust output units to 4\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "# Instantiate the BCNN model\n",
        "input_shape = (150, 150, 3)  # Assuming input shape is (150, 150, 3)\n",
        "bcnn_model = build_bcnn_model(input_shape)\n",
        "\n",
        "# Define and compile the Bayesian model\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)  # Adjust learning rate\n",
        "bcnn_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])  # Use binary crossentropy for multi-label classification\n",
        "\n",
        "# Train the BCNN model\n",
        "start_time = time()\n",
        "bcnn_history = bcnn_model.fit(x_train, y_train,\n",
        "                              batch_size=32,\n",
        "                              epochs=10,\n",
        "                              validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the BCNN model on test data\n",
        "test_loss, test_accuracy = bcnn_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {bcnn_history.history[\"accuracy\"][-1]}')  # Print final training accuracy\n",
        "print(f'Validation Acc: {bcnn_history.history[\"val_accuracy\"][-1]}')  # Print final validation accuracy\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8nTVdsmyIfe"
      },
      "source": [
        "**ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOO2ClM4E0LA",
        "outputId": "05296535-e76e-4aed-a00d-3ff35b9b9a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 309s 2s/step - loss: 0.5409 - accuracy: 0.4338 - val_loss: 0.5026 - val_accuracy: 0.5022\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 308s 2s/step - loss: 0.4819 - accuracy: 0.5408 - val_loss: 0.4715 - val_accuracy: 0.5739\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 303s 2s/step - loss: 0.4570 - accuracy: 0.5892 - val_loss: 0.4537 - val_accuracy: 0.6308\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 313s 2s/step - loss: 0.4400 - accuracy: 0.6225 - val_loss: 0.4414 - val_accuracy: 0.6045\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 328s 2s/step - loss: 0.4272 - accuracy: 0.6273 - val_loss: 0.4289 - val_accuracy: 0.6457\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 307s 2s/step - loss: 0.4145 - accuracy: 0.6500 - val_loss: 0.4171 - val_accuracy: 0.6273\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 328s 2s/step - loss: 0.4032 - accuracy: 0.6614 - val_loss: 0.4044 - val_accuracy: 0.6597\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 328s 2s/step - loss: 0.3936 - accuracy: 0.6748 - val_loss: 0.3960 - val_accuracy: 0.6588\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 326s 2s/step - loss: 0.3864 - accuracy: 0.6732 - val_loss: 0.3885 - val_accuracy: 0.6570\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 325s 2s/step - loss: 0.3761 - accuracy: 0.6927 - val_loss: 0.3799 - val_accuracy: 0.6710\n",
            "41/41 [==============================] - 74s 2s/step - loss: 0.4245 - accuracy: 0.6125\n",
            "Test Loss: 0.42452841997146606\n",
            "Test Accuracy: 0.612509548664093\n",
            "Time taken: 3209.234972715378 seconds\n",
            "Training Acc: 0.6927117705345154\n",
            "Validation Acc: 0.6710411310195923\n",
            "Test Acc: 0.612509548664093\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from time import time\n",
        "\n",
        "# Define ResNet50-based model\n",
        "def build_resnet_model(input_shape):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    predictions = layers.Dense(4, activation='sigmoid')(x)  # Adjust output units to 4\n",
        "    model = models.Model(inputs=base_model.input, outputs=predictions)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False  # Freeze pre-trained layers\n",
        "    return model\n",
        "\n",
        "# Instantiate the ResNet50-based model\n",
        "input_shape = (150, 150, 3)  # Assuming input shape is (150, 150, 3)\n",
        "resnet_model = build_resnet_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)  # Adjust learning rate if needed\n",
        "resnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time()\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)  # Add early stopping to prevent overfitting\n",
        "resnet_history = resnet_model.fit(x_train, y_train,\n",
        "                                  batch_size=32,\n",
        "                                  epochs=10,  # Increase epochs if needed\n",
        "                                  validation_data=(x_val, y_val),\n",
        "                                  callbacks=[early_stopping])\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = resnet_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {resnet_history.history[\"accuracy\"][-1]}')  # Print final training accuracy\n",
        "print(f'Validation Acc: {resnet_history.history[\"val_accuracy\"][-1]}')  # Print final validation accuracy\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ds1SuLkzYnz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}