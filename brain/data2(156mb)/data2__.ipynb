{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiDv0CZ6e0AG",
        "outputId": "ad831af4-6be6-4248-ba7c-a4c1c41b9e11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading brain-tumor-mri-dataset.zip to /content\n",
            " 87% 129M/149M [00:00<00:00, 313MB/s] \n",
            "100% 149M/149M [00:00<00:00, 310MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/brain-tumor-mri-dataset.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/brain_tumor_dataset')"
      ],
      "metadata": {
        "id": "VywDXyY5ogze"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, LambdaCallback\n",
        "from keras.layers import Input, Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.resnet import ResNet50"
      ],
      "metadata": {
        "id": "atn8qCGMowcD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define labels and image size\n",
        "labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "image_size = 150\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, 0)\n",
        "    image = cv2.bilateralFilter(image, 2, 50, 50)\n",
        "    image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    return image\n",
        "\n",
        "# Load images and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for label in labels:\n",
        "    trainPath = os.path.join('/content/brain_tumor_dataset/Training', label)\n",
        "    for file in tqdm(os.listdir(trainPath)):\n",
        "        image_path = os.path.join(trainPath, file)\n",
        "        x_train.append(preprocess_image(image_path))\n",
        "        y_train.append(labels.index(label))\n",
        "\n",
        "    testPath = os.path.join('/content/brain_tumor_dataset/Testing', label)\n",
        "    for file in tqdm(os.listdir(testPath)):\n",
        "        image_path = os.path.join(testPath, file)\n",
        "        x_test.append(preprocess_image(image_path))\n",
        "        y_test.append(labels.index(label))\n",
        "\n",
        "# Convert lists to NumPy arrays and normalize images\n",
        "x_train = np.array(x_train) / 255.0\n",
        "x_test = np.array(x_test) / 255.0\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Shuffle training data\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
        "\n",
        "# Perform one-hot encoding on labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pio7sLrLok8o",
        "outputId": "ce582d9d-590f-400c-cd23-79303a82a878"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1321/1321 [00:17<00:00, 74.54it/s]\n",
            "100%|██████████| 300/300 [00:02<00:00, 125.71it/s]\n",
            "100%|██████████| 1339/1339 [00:08<00:00, 161.79it/s]\n",
            "100%|██████████| 306/306 [00:01<00:00, 220.18it/s]\n",
            "100%|██████████| 1595/1595 [00:05<00:00, 293.83it/s]\n",
            "100%|██████████| 405/405 [00:01<00:00, 360.94it/s]\n",
            "100%|██████████| 1457/1457 [00:08<00:00, 172.77it/s]\n",
            "100%|██████████| 300/300 [00:01<00:00, 194.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple CNN**"
      ],
      "metadata": {
        "id": "pkJlsAcao2uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 3)),  # Adjust input shape for RGB images\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),  # You can adjust the learning rate if needed\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3gnD7_rhorNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from time import time\n",
        "\n",
        "# Train the model\n",
        "start_time = time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZBGqUkcpDYy",
        "outputId": "f8f3d1fb-0ccb-486c-b222-966fd95b7f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 9s 43ms/step - loss: 0.7382 - accuracy: 0.6973 - val_loss: 0.4519 - val_accuracy: 0.8241\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 4s 27ms/step - loss: 0.4582 - accuracy: 0.8350 - val_loss: 0.3371 - val_accuracy: 0.8845\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 4s 27ms/step - loss: 0.2907 - accuracy: 0.8919 - val_loss: 0.2896 - val_accuracy: 0.8915\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.2223 - accuracy: 0.9208 - val_loss: 0.2542 - val_accuracy: 0.9073\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.1678 - accuracy: 0.9416 - val_loss: 0.2206 - val_accuracy: 0.9265\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.1296 - accuracy: 0.9501 - val_loss: 0.2284 - val_accuracy: 0.9431\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 4s 29ms/step - loss: 0.1120 - accuracy: 0.9582 - val_loss: 0.2141 - val_accuracy: 0.9388\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 4s 30ms/step - loss: 0.0904 - accuracy: 0.9645 - val_loss: 0.2083 - val_accuracy: 0.9431\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.0702 - accuracy: 0.9746 - val_loss: 0.2535 - val_accuracy: 0.9309\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.0555 - accuracy: 0.9788 - val_loss: 0.2199 - val_accuracy: 0.9493\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 0.1986 - accuracy: 0.9535\n",
            "Test Loss: 0.19862478971481323\n",
            "Test Accuracy: 0.9534706473350525\n",
            "Time taken: 54.01562714576721 seconds\n",
            "Training Acc: 0.911271607875824\n",
            "Validation Acc: 0.9139107704162598\n",
            "Test Acc: 0.9534706473350525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TcKWXFHpDUw",
        "outputId": "8038670e-1e20-4252-e239-ff38458c8d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 0.055459994822740555\n",
            "Final Validation Loss: 0.21994422376155853\n",
            "The model is overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the MCD model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Monte Carlo Dropout\n",
        "def mc_dropout(y_true, y_pred):\n",
        "    return K.std(y_pred)\n",
        "\n",
        "model.compile(optimizer=Adam(),  # You can adjust the learning rate if needed\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', mc_dropout])  # Add mc_dropout as a metric for evaluation\n"
      ],
      "metadata": {
        "id": "hUtEWcqoppus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "# Train the model\n",
        "start_time = time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))\n",
        "end_time = time()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "QkmaYqXsqMPV",
        "outputId": "10c3aebb-8cfe-4dd7-a206-c96b43b9f041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 13s 44ms/step - loss: 0.7448 - accuracy: 0.7102 - mc_dropout: 0.2807 - val_loss: 0.4472 - val_accuracy: 0.8145 - val_mc_dropout: 0.3392\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.4298 - accuracy: 0.8468 - mc_dropout: 0.3536 - val_loss: 0.3243 - val_accuracy: 0.8889 - val_mc_dropout: 0.3782\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 4s 26ms/step - loss: 0.2825 - accuracy: 0.8963 - mc_dropout: 0.3797 - val_loss: 0.2419 - val_accuracy: 0.9090 - val_mc_dropout: 0.3941\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 4s 31ms/step - loss: 0.2149 - accuracy: 0.9225 - mc_dropout: 0.3937 - val_loss: 0.2197 - val_accuracy: 0.9283 - val_mc_dropout: 0.4044\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 4s 28ms/step - loss: 0.1674 - accuracy: 0.9365 - mc_dropout: 0.4016 - val_loss: 0.2248 - val_accuracy: 0.9195 - val_mc_dropout: 0.4033\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 4s 27ms/step - loss: 0.1266 - accuracy: 0.9532 - mc_dropout: 0.4084 - val_loss: 0.2139 - val_accuracy: 0.9370 - val_mc_dropout: 0.4143\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 4s 29ms/step - loss: 0.1053 - accuracy: 0.9637 - mc_dropout: 0.4142 - val_loss: 0.2085 - val_accuracy: 0.9405 - val_mc_dropout: 0.4154\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 4s 29ms/step - loss: 0.0970 - accuracy: 0.9634 - mc_dropout: 0.4146 - val_loss: 0.1796 - val_accuracy: 0.9431 - val_mc_dropout: 0.4142\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 4s 29ms/step - loss: 0.0778 - accuracy: 0.9733 - mc_dropout: 0.4189 - val_loss: 0.2069 - val_accuracy: 0.9405 - val_mc_dropout: 0.4174\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 4s 30ms/step - loss: 0.0641 - accuracy: 0.9777 - mc_dropout: 0.4206 - val_loss: 0.2337 - val_accuracy: 0.9449 - val_mc_dropout: 0.4205\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.2024 - accuracy: 0.9550 - mc_dropout: 0.4197\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-798b4089a162>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Evaluate the model on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "evaluation_results = model.evaluate(x_test, y_test)\n",
        "print(\"Evaluation Results:\", evaluation_results)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {evaluation_results[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI3WksLGrTFS",
        "outputId": "058819ee-2d51-4241-dfa6-ed041be9a87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 15ms/step - loss: 0.2024 - accuracy: 0.9550 - mc_dropout: 0.4197\n",
            "Evaluation Results: [0.2024240791797638, 0.9549961686134338, 0.41970425844192505]\n",
            "Time taken: 89.93361234664917 seconds\n",
            "Training Acc: 0.9143576323986053\n",
            "Validation Acc: 0.9166229128837585\n",
            "Test Acc: 0.9549961686134338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZenTJnGSqRn6",
        "outputId": "a8929449-fa34-48a7-a5e7-553a436277a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 0.0640702173113823\n",
            "Final Validation Loss: 0.23365411162376404\n",
            "The model is overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, regularizers, models, optimizers\n",
        "from time import time\n",
        "\n",
        "# Define Bayesian Neural Network model with increased capacity and regularization\n",
        "def build_bcnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(4, activation=\"sigmoid\")(x)  # Adjust output units to 4\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "# Instantiate the BCNN model\n",
        "input_shape = (150, 150, 3)  # Assuming input shape is (150, 150, 3)\n",
        "bcnn_model = build_bcnn_model(input_shape)\n",
        "\n",
        "# Define and compile the Bayesian model\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)  # Adjust learning rate\n",
        "bcnn_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])  # Use binary crossentropy for multi-label classification\n",
        "\n",
        "# Train the BCNN model\n",
        "start_time = time()\n",
        "bcnn_history = bcnn_model.fit(x_train, y_train,\n",
        "                              batch_size=32,\n",
        "                              epochs=10,\n",
        "                              validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the BCNN model on test data\n",
        "test_loss, test_accuracy = bcnn_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {bcnn_history.history[\"accuracy\"][-1]}')  # Print final training accuracy\n",
        "print(f'Validation Acc: {bcnn_history.history[\"val_accuracy\"][-1]}')  # Print final validation accuracy\n",
        "print(f'Test Acc: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrsk1VB9rf3m",
        "outputId": "7f166a4f-8d55-495d-df99-978cb8b4bee1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "143/143 [==============================] - 22s 94ms/step - loss: 0.6411 - accuracy: 0.7691 - val_loss: 1.2375 - val_accuracy: 0.2791\n",
            "Epoch 2/10\n",
            "143/143 [==============================] - 9s 60ms/step - loss: 0.4632 - accuracy: 0.8779 - val_loss: 1.0499 - val_accuracy: 0.3710\n",
            "Epoch 3/10\n",
            "143/143 [==============================] - 9s 60ms/step - loss: 0.3885 - accuracy: 0.9236 - val_loss: 0.5496 - val_accuracy: 0.7664\n",
            "Epoch 4/10\n",
            "143/143 [==============================] - 9s 62ms/step - loss: 0.3486 - accuracy: 0.9488 - val_loss: 0.4378 - val_accuracy: 0.8644\n",
            "Epoch 5/10\n",
            "143/143 [==============================] - 9s 65ms/step - loss: 0.3200 - accuracy: 0.9650 - val_loss: 0.3757 - val_accuracy: 0.9283\n",
            "Epoch 6/10\n",
            "143/143 [==============================] - 8s 59ms/step - loss: 0.2966 - accuracy: 0.9803 - val_loss: 0.4147 - val_accuracy: 0.8793\n",
            "Epoch 7/10\n",
            "143/143 [==============================] - 9s 61ms/step - loss: 0.2815 - accuracy: 0.9829 - val_loss: 0.3477 - val_accuracy: 0.9335\n",
            "Epoch 8/10\n",
            "143/143 [==============================] - 9s 61ms/step - loss: 0.2628 - accuracy: 0.9906 - val_loss: 0.3556 - val_accuracy: 0.9256\n",
            "Epoch 9/10\n",
            "143/143 [==============================] - 9s 60ms/step - loss: 0.2547 - accuracy: 0.9934 - val_loss: 0.3578 - val_accuracy: 0.9256\n",
            "Epoch 10/10\n",
            "143/143 [==============================] - 9s 62ms/step - loss: 0.2504 - accuracy: 0.9928 - val_loss: 0.3325 - val_accuracy: 0.9388\n",
            "41/41 [==============================] - 1s 36ms/step - loss: 0.3294 - accuracy: 0.9413\n",
            "Test Loss: 0.32944944500923157\n",
            "Test Accuracy: 0.9412662386894226\n",
            "Time taken: 147.19346237182617 seconds\n",
            "Training Acc: 0.992777407169342\n",
            "Validation Acc: 0.9387576580047607\n",
            "Test Acc: 0.9412662386894226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit(history):\n",
        "  # Get the final training and validation losses\n",
        "  final_train_loss = history.history['loss'][-1]\n",
        "  final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "  # Print the final losses\n",
        "  print(f'Final Training Loss: {final_train_loss}')\n",
        "  print(f'Final Validation Loss: {final_val_loss}')\n",
        "\n",
        "  # Check for overfitting\n",
        "  if final_val_loss > final_train_loss:\n",
        "      print('The model is overfitting.')\n",
        "  else:\n",
        "      print('The model is not overfitting.')"
      ],
      "metadata": {
        "id": "b-MSFGdEuRht"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(bcnn_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enHbXr43fxux",
        "outputId": "8a09e909-665f-495d-b160-391e279f62ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 0.25042837858200073\n",
            "Final Validation Loss: 0.33249351382255554\n",
            "The model is overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, regularizers, models, optimizers\n",
        "from time import time\n",
        "\n",
        "# Define Bayesian Neural Network model with increased capacity and regularization\n",
        "def build_bcnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.3)(x)  # Increase dropout rate\n",
        "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001))(x)  # Add a dense layer\n",
        "    x = layers.Dropout(0.6)(x)  # Add an additional dropout layer\n",
        "    outputs = layers.Dense(4, activation=\"sigmoid\")(x)  # Adjust output units to 4\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "# Instantiate the BCNN model\n",
        "input_shape = (150, 150, 3)  # Assuming input shape is (150, 150, 3)\n",
        "bcnn_model = build_bcnn_model(input_shape)\n",
        "\n",
        "# Define and compile the Bayesian model\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)  # Adjust learning rate\n",
        "bcnn_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])  # Use binary crossentropy for multi-label classification\n",
        "\n",
        "# Train the BCNN model\n",
        "start_time = time()\n",
        "bcnn_history = bcnn_model.fit(x_train, y_train,\n",
        "                              batch_size=32,\n",
        "                              epochs=10,\n",
        "                              validation_data=(x_val, y_val))\n",
        "end_time = time()\n",
        "\n",
        "# Evaluate the BCNN model on test data\n",
        "test_loss, test_accuracy = bcnn_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {bcnn_history.history[\"accuracy\"][-1]}')  # Print final training accuracy\n",
        "print(f'Validation Acc: {bcnn_history.history[\"val_accuracy\"][-1]}')  # Print final validation accuracy\n",
        "print(f'Test Acc: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "WSEELegxf30x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit(bcnn_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y216YpcdgU6i",
        "outputId": "b1db8a87-88fa-4160-bec6-097358a71cf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 2.8457884788513184\n",
            "Final Validation Loss: 2.734614372253418\n",
            "The model is not overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIBJZIW5gfcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}