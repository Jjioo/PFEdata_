{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hBMUcoeH4h9",
        "outputId": "409263e7-bad0-4721-b178-7ffb5be02a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.15.0\n",
            "1.25.2\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import numpy as np\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7KUL_kvYIN1e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection\n",
        "import zipfile\n",
        "zip_file_path = '/content/brain-mri-images-for-brain-tumor-detection.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/brain_tumor_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk7WP6J5IQtu",
        "outputId": "ed67b7d9-2800-4697-dd2b-0622e1f4dded"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection\n",
            "License(s): copyright-authors\n",
            "Downloading brain-mri-images-for-brain-tumor-detection.zip to /content\n",
            " 60% 9.00M/15.1M [00:00<00:00, 92.8MB/s]\n",
            "100% 15.1M/15.1M [00:00<00:00, 132MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import DenseNet121, VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Dense, GlobalAveragePooling2D, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import os\n",
        "import random\n",
        "import csv\n",
        "import shutil\n",
        "from keras.models import save_model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n"
      ],
      "metadata": {
        "id": "ewDKn-5VIT7_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "\n",
        "# Path to the data folder containing the two class folders (yes and no)\n",
        "data_folder = \"brain_tumor_dataset\"\n",
        "\n",
        "# List to store image paths and corresponding labels\n",
        "data = []\n",
        "\n",
        "# Loop through each class folder\n",
        "for class_label in [\"yes\", \"no\"]:\n",
        "    class_folder = os.path.join(data_folder, class_label)\n",
        "    # Get list of image files in the folder\n",
        "    images = os.listdir(class_folder)\n",
        "    # Iterate through each image\n",
        "    for img in images:\n",
        "        # Append image path and corresponding label to the data list\n",
        "        data.append((os.path.join(class_label, img), 1 if class_label == \"yes\" else 0))\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data)\n",
        "\n",
        "# Path to save the CSV file\n",
        "csv_file = \"data.csv\"\n",
        "\n",
        "# Write data to CSV file\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow(['Image', 'Label'])\n",
        "    # Write data rows\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"CSV file created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtzLcfXsV1fo",
        "outputId": "40b21b8e-1424-4219-9fc7-2bc4a23bf426"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data from a CSV file\n",
        "def load_data_from_csv(csv_file):\n",
        "    data = []\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            data.append(row)\n",
        "    return data\n",
        "\n",
        "# Function to shuffle and split data into train, validation, and test sets\n",
        "def split_data(data, test_ratio=0.3, validation_ratio=0.2):\n",
        "    random.shuffle(data)\n",
        "    test_size = int(test_ratio * len(data))\n",
        "    validation_size = int(validation_ratio * len(data))\n",
        "    test_data = data[:test_size]\n",
        "    remaining_data = data[test_size:]\n",
        "    validation_data = remaining_data[:validation_size]\n",
        "    train_data = remaining_data[validation_size:]\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "# Function to copy images to destination folder\n",
        "def copy_images(data, destination_folder, source_folder=\"brain_tumor_dataset\"):\n",
        "    for row in data:\n",
        "        image_path = row[0]\n",
        "        label = row[1]\n",
        "        class_folder = os.path.join(destination_folder, \"yes\" if label == '1' else \"no\")\n",
        "        os.makedirs(class_folder, exist_ok=True)\n",
        "        shutil.copy(os.path.join(source_folder, image_path), class_folder)\n",
        "\n",
        "# Load data from CSV file\n",
        "csv_file = \"data.csv\"\n",
        "data = load_data_from_csv(csv_file)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_data, validation_data, test_data = split_data(data)\n",
        "\n",
        "# Create folders for train, validation, and test data\n",
        "train_folder = \"Training\"\n",
        "validation_folder = \"Validation\"\n",
        "test_folder = \"Testing\"\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(validation_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "# Copy train, validation, and test images to respective folders\n",
        "copy_images(train_data, train_folder)\n",
        "copy_images(validation_data, validation_folder)\n",
        "copy_images(test_data, test_folder)\n",
        "\n",
        "print(\"Train, validation, and test data copied to respective folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ogBN7s1IZ5K",
        "outputId": "a02aebbd-1261-40b0-f128-2a1f0ced0195"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train, validation, and test data copied to respective folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 64\n",
        "input_shape = (img_width, img_height, 3)\n",
        "num_classes = 2  # For binary classification\n",
        "\n",
        "# Set up data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Set up train, validation, and test generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'Training',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'Validation',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'Testing',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # Don't shuffle test data for evaluation\n",
        ")\n",
        "\n",
        "# Print class indices for reference\n",
        "print(\"Class Indices:\")\n",
        "print(train_generator.class_indices)\n",
        "\n",
        "# Fetch batches of data directly from generators\n",
        "X_train, y_train = next(train_generator)\n",
        "X_val, y_val = next(validation_generator)\n",
        "X_test, y_test = next(test_generator)\n",
        "\n",
        "# Resize the images\n",
        "X_train = tf.image.resize(X_train, (28, 28))\n",
        "X_val = tf.image.resize(X_val, (28, 28))\n",
        "X_test = tf.image.resize(X_test, (28, 28))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I90tOg3RIcsz",
        "outputId": "723d0b38-0831-4308-bacc-96447fd3f29e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 128 images belonging to 2 classes.\n",
            "Found 50 images belonging to 2 classes.\n",
            "Found 75 images belonging to 2 classes.\n",
            "Class Indices:\n",
            "{'no': 0, 'yes': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit(history):\n",
        "  # Get the final training and validation losses\n",
        "  final_train_loss = history.history['loss'][-1]\n",
        "  final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "  # Print the final losses\n",
        "  print(f'Final Training Loss: {final_train_loss}')\n",
        "  print(f'Final Validation Loss: {final_val_loss}')\n",
        "\n",
        "  # Check for overfitting\n",
        "  if final_val_loss > final_train_loss:\n",
        "      print('The model is overfitting.')\n",
        "  else:\n",
        "      print('The model is not overfitting.')"
      ],
      "metadata": {
        "id": "tAwvm_ugIi7k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dropout, Input\n",
        "\n",
        "class MonteCarloDropoutImproved:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def build_model(self, model_file_path, dropout_rate=0.5):\n",
        "        model = self.__load_model(model_file_path)\n",
        "        input_layer = model.input\n",
        "        x = input_layer\n",
        "        index = 0\n",
        "\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
        "                x = Dropout(dropout_rate, name=f\"dropout_{index}\")(x, training=True)\n",
        "                index += 1\n",
        "            x = layer(x)\n",
        "\n",
        "        self.model = Model(input_layer, x)\n",
        "\n",
        "    def predict_monte_carlo_dropout(self, input_samples, sampling_num):\n",
        "        predictions = []\n",
        "\n",
        "        for _ in range(sampling_num):\n",
        "            pred = self.model.predict(input_samples)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        pred_avg = np.average(predictions, axis=0)\n",
        "        pred_std = np.std(predictions, axis=0)\n",
        "\n",
        "        return pred_avg, pred_std\n",
        "\n",
        "    def frequency_distribution(self, input_sample, sampling_num):\n",
        "        input_samples = np.tile(input_sample, (sampling_num, 1, 1, 1))\n",
        "        predictions = self.model.predict(input_samples)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def __load_model(self, model_file_path):\n",
        "        model = tf.keras.models.load_model(model_file_path)\n",
        "        return model"
      ],
      "metadata": {
        "id": "_qAdjHXk2Q2U"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "class VariationalInferenceNN:\n",
        "    def __init__(self):\n",
        "        self.variational_model = None\n",
        "\n",
        "    def build_variational_model(self, base_model):\n",
        "        inputs = base_model.input\n",
        "        outputs = base_model.output\n",
        "        num_samples = tf.placeholder_with_default(input=1, shape=())\n",
        "\n",
        "        def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
        "            n = kernel_size + bias_size\n",
        "            c = tf.math.log(tf.expm1(1.))\n",
        "            return tf.keras.Sequential([\n",
        "                tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
        "                tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "                    tfd.Normal(loc=t[..., :n],\n",
        "                               scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
        "                    reinterpreted_batch_ndims=1)),\n",
        "            ])\n",
        "\n",
        "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros_like(tf.reshape(outputs, (-1,))),\n",
        "                                           scale=1.0), reinterpreted_batch_ndims=1)\n",
        "\n",
        "        posterior = posterior_mean_field(kernel_size=outputs.shape[-1], dtype=tf.float32)\n",
        "\n",
        "        y = tfp.layers.DenseVariational(outputs.shape[-1], posterior, prior, kl_weight=num_samples / tf.cast(50, tf.float32))(inputs)\n",
        "        variational_model = Model(inputs=[inputs, num_samples], outputs=y)\n",
        "        self.variational_model = variational_model\n",
        "\n",
        "    def compile_model(self, optimizer='adam', loss='mse'):\n",
        "        if self.variational_model is None:\n",
        "            raise ValueError(\"Variational model is not built yet. Call build_variational_model() first.\")\n",
        "        self.variational_model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    def fit(self, x_train, y_train, num_samples=10, epochs=10, batch_size=32, validation_data=None):\n",
        "        if self.variational_model is None:\n",
        "            raise ValueError(\"Variational model is not built yet. Call build_variational_model() first.\")\n",
        "        self.variational_model.fit([x_train, num_samples], y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        if self.variational_model is None:\n",
        "            raise ValueError(\"Variational model is not built yet. Call build_variational_model() first.\")\n",
        "        return self.variational_model.evaluate([x_test, 1], y_test)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(model_file_path):\n",
        "        return tf.keras.models.load_model(model_file_path)\n"
      ],
      "metadata": {
        "id": "aFFu-pf6mhZ7"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple CNN**"
      ],
      "metadata": {
        "id": "0L5LdqwmtyaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_simple_cnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),  # Adding dropout regularization\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create the modified model\n",
        "model = create_simple_cnn(input_shape, num_classes)\n",
        "\n",
        "# Compile the model with reduced learning rate and early stopping\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model with the early stopping callback\n",
        "start_time = time.time()\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(train_generator),\n",
        "                    epochs=10,  # Increased epochs for better convergence\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=len(validation_generator),\n",
        "                    callbacks=[early_stopping])  # Using early stopping\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiXWdNqRouXD",
        "outputId": "f4903aa2-1608-489a-91d0-26a787b80874"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6668 - accuracy: 0.5729"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7d74e1e64f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 3s 201ms/step - loss: 0.7710 - accuracy: 0.5781 - val_loss: 0.5231 - val_accuracy: 0.7600\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 1s 140ms/step - loss: 0.6313 - accuracy: 0.7188 - val_loss: 0.5208 - val_accuracy: 0.7733\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 1s 151ms/step - loss: 0.5416 - accuracy: 0.7656 - val_loss: 0.4734 - val_accuracy: 0.7867\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 1s 150ms/step - loss: 0.4550 - accuracy: 0.8047 - val_loss: 0.4607 - val_accuracy: 0.7467\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 1s 150ms/step - loss: 0.4039 - accuracy: 0.8281 - val_loss: 0.4411 - val_accuracy: 0.8133\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 1s 147ms/step - loss: 0.3715 - accuracy: 0.8672 - val_loss: 0.4385 - val_accuracy: 0.8133\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 1s 141ms/step - loss: 0.3371 - accuracy: 0.8750 - val_loss: 0.4270 - val_accuracy: 0.8133\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 1s 160ms/step - loss: 0.2786 - accuracy: 0.8906 - val_loss: 0.4256 - val_accuracy: 0.8000\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 1s 147ms/step - loss: 0.2490 - accuracy: 0.9062 - val_loss: 0.4141 - val_accuracy: 0.8133\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 1s 144ms/step - loss: 0.2340 - accuracy: 0.9297 - val_loss: 0.4241 - val_accuracy: 0.8267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_time = end_time - start_time\n",
        "\n",
        "# Print training time\n",
        "print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Print Training and Validation Accuracy\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "print(f'Training Accuracy: {train_acc}')\n",
        "print(f'Validation Accuracy: {val_acc}')\n",
        "\n",
        "# Check for overfitting\n",
        "overfit(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxFzziRtuFSD",
        "outputId": "645762dd-4fd1-4373-c701-9a061c3e7052"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 8.206925868988037 seconds\n",
            "Test accuracy: 0.8266666531562805\n",
            "Training Accuracy: 0.9296875\n",
            "Validation Accuracy: 0.8266666531562805\n",
            "Final Training Loss: 0.23398305475711823\n",
            "Final Validation Loss: 0.42405518889427185\n",
            "The model is overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCx9TN-nmmFy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model\n",
        "model_path = '/content/models/simple_cnn_model.h5'\n",
        "save_model(model, model_path)\n",
        "# Create an instance of MonteCarloDropout and load the saved model\n",
        "mcd_model = MonteCarloDropoutImproved()\n",
        "mcd_model.build_model(model_path)\n",
        "\n",
        "# Load your original model for comparison and get the input shape\n",
        "original_model = keras.models.load_model(model_path)\n",
        "original_input_shape = original_model.input_shape\n",
        "\n",
        "# Resize the test images to match the input shape of the original model\n",
        "resized_X_test = tf.image.resize(X_test, (224, 224))\n",
        "\n",
        "original_loss, original_accuracy = original_model.evaluate(resized_X_test, y_test)\n",
        "\n",
        "# Set the number of Monte Carlo Dropout samples\n",
        "sampling_num = 100\n",
        "\n",
        "# Get true labels (assuming from a generator like test_generator)\n",
        "true_labels = test_generator[0][1]\n",
        "\n",
        "pred_avg, pred_std = mcd_model.predict_monte_carlo_dropout(resized_X_test, sampling_num)\n",
        "\n",
        "pred_labels = np.argmax(pred_avg, axis=1)\n",
        "new_accuracy = np.mean(pred_labels == true_labels)\n"
      ],
      "metadata": {
        "id": "bvD3OMKC341y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6419e94d-b23e-44e5-ea5f-0a573282b9c9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5628 - accuracy: 0.7656\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n"
          ]
        }
      ]
    },
    {
      "source": [
        "print('new_accuracy',new_accuracy)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx0ccN8vyIxL",
        "outputId": "7913c063-a7d4-45b5-914f-ac0224daad4b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_accuracy 0.640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Constants\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "input_shape = (img_width, img_height, 3)\n",
        "epochs = 10\n",
        "\n",
        "# Set up data generators\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/Training',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    '/content/Testing',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC4pnzSqZ1ig",
        "outputId": "454cf13b-026a-4730-9289-ef37f079102f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 128 images belonging to 2 classes.\n",
            "Found 75 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Resize test_generator.labels to match resized_X_test\n",
        "if resized_X_test.shape[0] != len(test_generator.labels):\n",
        "    new_labels = np.array([label for label, _ in zip(test_generator.labels, range(resized_X_test.shape[0]))])\n",
        "    true_labels = new_labels  # Update true_labels with the resized labels\n",
        "else:\n",
        "    true_labels = test_generator.labels  # Use the original labels\n",
        "\n",
        "# Resize validation_generator.labels to match X_val\n",
        "if X_val.shape[0] != len(validation_generator.labels):\n",
        "    new_val_labels = np.array(validation_generator.labels[:X_val.shape[0]])\n",
        "else:\n",
        "    new_val_labels = validation_generator.labels\n",
        "\n",
        "# Measure time taken for Bayesian method\n",
        "start_time_bayesian = time.time()\n",
        "\n",
        "# Set the number of Monte Carlo Dropout samples\n",
        "sampling_num = 100\n",
        "\n",
        "pred_avg, pred_std = mcd_model.predict_monte_carlo_dropout(resized_X_test, sampling_num)\n",
        "resized_X_val = tf.image.resize(X_val, (224, 224))  # Adjust the size as needed\n",
        "\n",
        "# Calculate new accuracy\n",
        "pred_labels = np.argmax(pred_avg, axis=1)\n",
        "new_accuracy = np.mean(pred_labels == true_labels)\n",
        "\n",
        "# Calculate validation accuracy and test accuracy for Bayesian method\n",
        "val_pred_labels = np.argmax(mcd_model.predict_monte_carlo_dropout(resized_X_val, sampling_num), axis=1)\n",
        "# Ensure new_val_labels has compatible shape for comparison\n",
        "val_accuracy_bayesian = np.mean(val_pred_labels == new_val_labels[:len(val_pred_labels)])  # Use slicing to match shapes\n",
        "\n",
        "test_accuracy_bayesian = np.mean(pred_labels == true_labels)\n",
        "\n",
        "# Measure time taken for Bayesian method\n",
        "end_time_bayesian = time.time()\n",
        "bayesian_method_time = end_time_bayesian - start_time_bayesian\n",
        "\n",
        "# Calculate composite score\n",
        "time_weight = 0.4  # 40% weight for time\n",
        "train_acc_weight = 0.1  # 10% weight for training accuracy\n",
        "val_acc_weight = 0.3  # 30% weight for validation accuracy\n",
        "test_acc_weight = 0.2  # 20% weight for test accuracy\n",
        "\n",
        "composite_score = (\n",
        "    (1 / bayesian_method_time) * time_weight +\n",
        "    train_acc * train_acc_weight +\n",
        "    val_accuracy_bayesian * val_acc_weight +\n",
        "    test_accuracy_bayesian * test_acc_weight\n",
        ")\n",
        "\n",
        "# Print Bayesian method results and composite score\n",
        "print(f\"New Accuracy (Bayesian Method): {new_accuracy}\")\n",
        "print(f\"Validation Accuracy (Bayesian Method): {val_accuracy_bayesian}\")\n",
        "print(f\"Test Accuracy (Bayesian Method): {test_accuracy_bayesian}\")\n",
        "print(f\"Time taken for Bayesian Method: {bayesian_method_time} seconds\")\n",
        "print(f\"Composite Score (Bayesian Method): {composite_score}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ncdQP73xyKYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1972ee9d-c839-4e59-aa0d-a9b4e94b5f88"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "New Accuracy (Bayesian Method): 0.640625\n",
            "Validation Accuracy (Bayesian Method): 0.0\n",
            "Test Accuracy (Bayesian Method): 0.640625\n",
            "Time taken for Bayesian Method: 21.77505397796631 seconds\n",
            "Composite Score (Bayesian Method): 0.2394633944750379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resized_X_train = tf.image.resize(X_train, (224, 224))  # Adjust the size as needed"
      ],
      "metadata": {
        "id": "0DcTb-IKjj8R"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Assuming y_train and y_val are your label arrays\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=2)\n",
        "y_val_one_hot = to_categorical(y_val, num_classes=2)\n",
        "\n",
        "# Build and compile your model\n",
        "mcd_model = MonteCarloDropoutImproved()\n",
        "mcd_model.build_model(model_file_path='/content/models/simple_cnn_model.h5')  # Replace 'your_model_path.h5' with your model file\n",
        "mcd_model.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Now you can evaluate your model using the resized input data and one-hot encoded labels\n",
        "train_loss, train_accuracy = mcd_model.model.evaluate(resized_X_train, y_train_one_hot, verbose=0)\n",
        "val_loss, val_accuracy = mcd_model.model.evaluate(resized_X_val, y_val_one_hot, verbose=0)\n",
        "\n",
        "# Check for overfitting\n",
        "if train_accuracy > val_accuracy:\n",
        "    print(\"Warning: Potential overfitting detected.\")\n",
        "else:\n",
        "    print(\"No overfitting detected.\")\n",
        "\n",
        "# Print training and validation accuracies\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIcdM1LOihfp",
        "outputId": "306edd73-5447-4c88-82b7-d0aeca9827ee"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Potential overfitting detected.\n",
            "Training Accuracy: 0.609375\n",
            "Validation Accuracy: 0.5799999833106995\n"
          ]
        }
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "SyyGop9XyLE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes by Backprop**"
      ],
      "metadata": {
        "id": "qI5YzDoZJGvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(0.25)(x)  # Add dropout after Conv2D layer\n",
        "x = Conv2D(64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(0.25)(x)  # Add dropout after Conv2D layer\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # Add dropout after Dense layer\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "\n",
        "# Get the final training and validation losses\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "# Print the final losses\n",
        "print(f'Final Training Loss: {final_train_loss}')\n",
        "print(f'Final Validation Loss: {final_val_loss}')\n",
        "\n",
        "# Check for overfitting\n",
        "if final_val_loss > final_train_loss:\n",
        "    print('The model is overfitting.')\n",
        "else:\n",
        "    print('The model is not overfitting.')\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = model.evaluate(train_generator)[1]\n",
        "val_acc = model.evaluate(validation_generator)[1]\n",
        "test_acc = model.evaluate(test_generator)[1]\n",
        "\n",
        "print(f'Training Acc: {train_acc}')\n",
        "print(f'Validation Acc: {val_acc}')\n",
        "print(f'Test Acc: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4PmhlS9T7Xm",
        "outputId": "313d3be4-cf08-4939-a7b8-0554b64235cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 128 images belonging to 2 classes.\n",
            "Found 50 images belonging to 2 classes.\n",
            "Found 75 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 1s/step - loss: 1.5790 - accuracy: 0.5547 - val_loss: 0.4904 - val_accuracy: 0.7800\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 1s 615ms/step - loss: 0.9043 - accuracy: 0.5312 - val_loss: 0.7860 - val_accuracy: 0.3800\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 1s 608ms/step - loss: 0.8881 - accuracy: 0.5547 - val_loss: 0.4390 - val_accuracy: 0.7800\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 1s 614ms/step - loss: 0.7717 - accuracy: 0.6797 - val_loss: 0.4375 - val_accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 1s 410ms/step - loss: 0.7787 - accuracy: 0.6875 - val_loss: 0.4406 - val_accuracy: 0.8200\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 1s 419ms/step - loss: 0.5816 - accuracy: 0.7109 - val_loss: 0.5271 - val_accuracy: 0.7600\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 1s 403ms/step - loss: 0.6046 - accuracy: 0.6719 - val_loss: 0.5834 - val_accuracy: 0.7600\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 1s 416ms/step - loss: 0.6257 - accuracy: 0.6172 - val_loss: 0.5425 - val_accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 1s 395ms/step - loss: 0.5756 - accuracy: 0.6562 - val_loss: 0.5001 - val_accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 1s 392ms/step - loss: 0.5372 - accuracy: 0.7188 - val_loss: 0.4737 - val_accuracy: 0.8200\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.5500 - accuracy: 0.7333\n",
            "Test accuracy: 0.7333333492279053\n",
            "Final Training Loss: 0.5372179746627808\n",
            "Final Validation Loss: 0.473704993724823\n",
            "The model is not overfitting.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.5500 - accuracy: 0.7333\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.5531 - accuracy: 0.7578\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4737 - accuracy: 0.8200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5500 - accuracy: 0.7333\n",
            "Training Acc: 0.7578125\n",
            "Validation Acc: 0.8199999928474426\n",
            "Test Acc: 0.7333333492279053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D cnn**"
      ],
      "metadata": {
        "id": "UTE4MJc0fM4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time} seconds\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Print process time and metrics\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "print(f'Training Acc: {np.mean(history.history[\"accuracy\"])}')\n",
        "print(f'Validation Acc: {np.mean(history.history[\"val_accuracy\"])}')\n",
        "print(f'Test Acc: {test_acc}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4JniDBSY9on",
        "outputId": "c381f84d-3509-4ce3-df01-79d08e55cf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 2s/step - loss: 7.9992 - accuracy: 0.4219 - val_loss: 0.8350 - val_accuracy: 0.7000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 1s 585ms/step - loss: 4.3070 - accuracy: 0.4844 - val_loss: 5.3258 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 1s 334ms/step - loss: 3.3262 - accuracy: 0.4297 - val_loss: 1.7409 - val_accuracy: 0.3000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 1.1170 - accuracy: 0.4297 - val_loss: 0.7795 - val_accuracy: 0.3000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.6696 - accuracy: 0.5469 - val_loss: 0.6237 - val_accuracy: 0.8600\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 1s 383ms/step - loss: 0.6343 - accuracy: 0.7578 - val_loss: 0.6120 - val_accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.6217 - accuracy: 0.7891 - val_loss: 0.5802 - val_accuracy: 0.8600\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 1s 393ms/step - loss: 0.5690 - accuracy: 0.7578 - val_loss: 0.5209 - val_accuracy: 0.8000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 1s 385ms/step - loss: 0.5143 - accuracy: 0.7344 - val_loss: 0.3834 - val_accuracy: 0.8400\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 0.4787 - accuracy: 0.7891 - val_loss: 0.3819 - val_accuracy: 0.8400\n",
            "Training Time: 11.858389139175415 seconds\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.5437 - accuracy: 0.7867\n",
            "Test accuracy: 0.7866666913032532\n",
            "Time taken: 11.858389139175415 seconds\n",
            "Training Acc: 0.6140625\n",
            "Validation Acc: 0.6639999985694885\n",
            "Test Acc: 0.7866666913032532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6MEaKmtaS9o",
        "outputId": "881bf2bd-441b-41cd-981a-a201ee80313f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Loss: 0.47866290807724\n",
            "Final Validation Loss: 0.38188162446022034\n",
            "The model is not overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "8F93_5oRfzMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monte Carlo Dropout**"
      ],
      "metadata": {
        "id": "YVyBVuzJgUJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "class MontecarloDropout:\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def build_model(self, model_file_path):\n",
        "        \"\"\"\n",
        "        keras modelからMontecarloDropoutに対応したモデルを作成\n",
        "        build monte carlo dropout model base on keras_model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = self.__load_model(model_file_path)\n",
        "\n",
        "        # change dropout layer to dropout layer that can use dropout in inference.\n",
        "        # ドロップアウト層を推論時にもドロップアウトできるドロップアウト層に変更する。\n",
        "        for ily, layer in enumerate(model.layers):\n",
        "            # input layer\n",
        "            if ily == 0:\n",
        "                input = layer.input\n",
        "                h = input\n",
        "            # is dropout layer ?\n",
        "            if 'dropout' in layer.name:\n",
        "                # change dropout layer\n",
        "                h = Dropout(layer.rate)(h, training=True)\n",
        "            else:\n",
        "                h = layer(h)\n",
        "\n",
        "        self.model = Model(input, h)\n",
        "\n",
        "        return\n",
        "\n",
        "    def md_predict(self, xs, sampling_num):\n",
        "        \"\"\"\n",
        "        predict with using monte carlo dropout sampling.\n",
        "        return prediction average, std\n",
        "\n",
        "        xs : input sample array. xs = x0, x1, x2, ...\n",
        "        \"\"\"\n",
        "\n",
        "        pre_ys = []\n",
        "        for ismp in range(sampling_num):\n",
        "            pre_y = self.model.predict(xs)\n",
        "            pre_ys.append(pre_y)\n",
        "        pre_ys = np.array(pre_ys)\n",
        "\n",
        "        # calculate ave, std\n",
        "        pre_ave = np.average(pre_ys, axis=0)\n",
        "        pre_std = np.std(pre_ys, axis=0)\n",
        "\n",
        "        return pre_ave, pre_std\n",
        "\n",
        "    def md_freq_dist(self, x, sampling_num):\n",
        "        \"\"\"\n",
        "        frequency distribution of prediction with using monte carlo dropout sampling.\n",
        "        return predict(x), predict(x), ..., predict(x)\n",
        "\n",
        "        x : one input sample\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        xs = np.ones((sampling_num, *x.shape)) * x\n",
        "        pre_y = self.model.predict(xs)\n",
        "\n",
        "        return pre_y\n",
        "\n",
        "    def __load_model(self, model_file_path):\n",
        "        \"\"\"\n",
        "        load model .h5 file\n",
        "        \"\"\"\n",
        "        model = keras.models.load_model(model_file_path)\n",
        "        return model"
      ],
      "metadata": {
        "id": "WFLoPb6qlcSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugaHRMoGld9h",
        "outputId": "c64ea0b5-f955-4e7e-830e-ef952fbb2026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 4.8896 - accuracy: 0.5312 - val_loss: 4.9059 - val_accuracy: 0.3000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 1s 390ms/step - loss: 2.4503 - accuracy: 0.5781 - val_loss: 0.4002 - val_accuracy: 0.8600\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 1s 400ms/step - loss: 0.8714 - accuracy: 0.7812 - val_loss: 0.3188 - val_accuracy: 0.8400\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 1s 384ms/step - loss: 0.5648 - accuracy: 0.7266 - val_loss: 0.7021 - val_accuracy: 0.6200\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 1s 485ms/step - loss: 0.4790 - accuracy: 0.7266 - val_loss: 0.4067 - val_accuracy: 0.8000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 1s 556ms/step - loss: 0.3091 - accuracy: 0.8906 - val_loss: 0.2872 - val_accuracy: 0.9200\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 1s 589ms/step - loss: 0.3072 - accuracy: 0.8906 - val_loss: 0.2671 - val_accuracy: 0.9200\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 0.2159 - accuracy: 0.9453 - val_loss: 0.6067 - val_accuracy: 0.6800\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 1s 396ms/step - loss: 0.2278 - accuracy: 0.9219 - val_loss: 0.2184 - val_accuracy: 0.9200\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 1s 388ms/step - loss: 0.1654 - accuracy: 0.9219 - val_loss: 0.2434 - val_accuracy: 0.9200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b8add9e5ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('2D_CNN.h5')\n"
      ],
      "metadata": {
        "id": "VH1MARWTl46G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s1q96lrKmBNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAco0t-7mOjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcd = MontecarloDropout()\n",
        "mcd.build_model('2D_CNN.h5')\n",
        "\n",
        "# Make predictions with Monte Carlo Dropout on test data\n",
        "pre_ave, pre_std = mcd.md_predict(test_generator[0][0], sampling_num=10)\n",
        "\n",
        "# Ground truth labels for the test data\n",
        "true_labels = test_generator[0][1]\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "binary_predictions = (pre_ave > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = np.mean(binary_predictions == true_labels)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Test Acc: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbtu1SyohAmT",
        "outputId": "e3ca267d-b4af-4bce-8548-981eeb00aa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 11ms/step\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Test Acc: 56.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Te_niNSvmS_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}